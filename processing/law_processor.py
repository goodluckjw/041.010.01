import requests
import xml.etree.ElementTree as ET
from urllib.parse import quote
import re

OC = "chetera"
BASE = "http://www.law.go.kr"

def get_law_list_from_api(query):
    encoded_query = quote(query.split("&")[0].split(",")[0].split("-")[0].strip())
    url = f"{BASE}/DRF/lawSearch.do?OC={OC}&target=law&type=XML&display=100&search=2&knd=A0002&query={encoded_query}"
    res = requests.get(url, timeout=10)
    res.encoding = 'utf-8'
    laws = []
    if res.status_code == 200:
        root = ET.fromstring(res.content)
        for law in root.findall("law"):
            laws.append({
                "ë²•ë ¹ëª…": law.findtext("ë²•ë ¹ëª…í•œê¸€", "").strip(),
                "MST": law.findtext("ë²•ë ¹ì¼ë ¨ë²ˆí˜¸", ""),
                "URL": BASE + law.findtext("ë²•ë ¹ìƒì„¸ë§í¬", "")
            })
    return laws

def get_law_text_by_mst(mst):
    url = f"{BASE}/DRF/lawService.do?OC={OC}&target=law&MST={mst}&type=XML"
    res = requests.get(url, timeout=10)
    res.encoding = 'utf-8'
    return res.content if res.status_code == 200 else None

def clean(text):
    return re.sub(r"\s+", "", text or "")

def highlight(text, terms):
    if not text:
        return ""
    for term in terms:
        text = text.replace(term, f"<span style='color:red'>{term}</span>")
    return text

def logic_match(text, query):
    text = clean(text)
    include = [t.strip() for t in re.split(r"[,&\-]", query) if not t.startswith("-") and t.strip() in text]
    exclude = [t[1:].strip() for t in query.split() if t.startswith("-") and t[1:] in text]
    return all(word in text for word in include) and not any(word in text for word in exclude)

def get_highlighted_articles(mst, query):
    xml_data = get_law_text_by_mst(mst)
    if not xml_data:
        return "âš ï¸ ë³¸ë¬¸ì„ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    tree = ET.fromstring(xml_data)
    articles = tree.findall(".//ì¡°ë¬¸ë‹¨ìœ„")
    terms = [t.strip() for t in re.split(r"[,&\-]", query) if t.strip()]
    results = []

    for article in articles:
        ì¡°ë²ˆí˜¸ = article.findtext("ì¡°ë¬¸ë²ˆí˜¸", "").strip()
        ì¡°ì œëª© = article.findtext("ì¡°ë¬¸ì œëª©", "") or ""
        ì¡°ë‚´ìš© = article.findtext("ì¡°ë¬¸ë‚´ìš©", "") or ""
        í•­ë“¤ = article.findall("í•­")

        ì¡°ì¶œë ¥ = logic_match(ì¡°ì œëª© + ì¡°ë‚´ìš©, query)
        í•­ì¶œë ¥ = []

        for í•­ in í•­ë“¤:
            í•­ë²ˆí˜¸ = í•­.findtext("í•­ë²ˆí˜¸", "").strip()
            í•­ë‚´ìš© = í•­.findtext("í•­ë‚´ìš©", "") or ""
            í˜¸ì¶œë ¥ = []

            if logic_match(í•­ë‚´ìš©, query):
                ì¡°ì¶œë ¥ = True

            for í˜¸ in í•­.findall("í˜¸"):
                í˜¸ë‚´ìš© = í˜¸.findtext("í˜¸ë‚´ìš©", "") or ""
                if logic_match(í˜¸ë‚´ìš©, query):
                    ì¡°ì¶œë ¥ = True
                    í˜¸ì¶œë ¥.append(f"{highlight(í˜¸ë‚´ìš©, terms)}")
                for ëª© in í˜¸.findall("ëª©"):
                    ëª©ë‚´ìš© = ëª©.findtext("ëª©ë‚´ìš©", "") or ""
                    if logic_match(ëª©ë‚´ìš©, query):
                        ì¡°ì¶œë ¥ = True
                        í˜¸ì¶œë ¥.append(f"&nbsp;&nbsp;{highlight(ëª©ë‚´ìš©, terms)}")

            for ëª© in í•­.findall("ëª©"):
                ëª©ë‚´ìš© = ëª©.findtext("ëª©ë‚´ìš©", "") or ""
                if logic_match(ëª©ë‚´ìš©, query):
                    ì¡°ì¶œë ¥ = True
                    í˜¸ì¶œë ¥.append(f"&nbsp;&nbsp;{highlight(ëª©ë‚´ìš©, terms)}")

            if logic_match(í•­ë‚´ìš©, query) or í˜¸ì¶œë ¥:
                í•­ì¶œë ¥.append(f"â“{í•­ë²ˆí˜¸} {highlight(í•­ë‚´ìš©, terms)}<br>" + "<br>".join(í˜¸ì¶œë ¥))

        if ì¡°ì¶œë ¥ or í•­ì¶œë ¥:
            output = f"ì œ{ì¡°ë²ˆí˜¸}ì¡°({ì¡°ì œëª©})"
            if not í•­ë“¤:
                output += f" {highlight(ì¡°ë‚´ìš©, terms)}"
            else:
                output += "<br>" + "<br>".join(í•­ì¶œë ¥)
            results.append(output)

    return "<br><br>".join(results) if results else "ğŸ” í•´ë‹¹ ê²€ìƒ‰ì–´ë¥¼ í¬í•¨í•œ ì¡°ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤."
